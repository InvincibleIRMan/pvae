{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "############### data loading ######################\n",
      "data/train_zinc.txt\n",
      "data/valid_zinc.txt\n",
      "data/test_zinc.txt\n",
      "Number of SMILES >>>>  220011\n"
     ]
    }
   ],
   "source": [
    "from model_property2 import *\n",
    "from data_prepration import *\n",
    "from params import *\n",
    "from multiprocessing import cpu_count\n",
    "import numpy as np\n",
    "import pickle\n",
    "from util import *\n",
    "import pickle \n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import json\n",
    "Args = Params()\n",
    "\n",
    "print('############### data loading ######################')\n",
    "data_dir = 'data/'\n",
    "data_list = ['train_zinc','valid_zinc','test_zinc']\n",
    "\n",
    "data_train,data_valid,data_test = text2dict_zinc(data_dir, data_list)\n",
    "#txt, data_train_ = smi_postprocessing(data_train,biology_flag=False,LB_smi=15,UB_smi=120)\n",
    "\n",
    "print(\"Number of SMILES >>>> \" , len(data_train['SMILES']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smiles preprocessing:\n",
    "'''\n",
    "We filter the SMILES based on two criterias:\n",
    "1) 8 < logP < -3 and 600 < MolecularWeight < 50,\n",
    "2) 120 < smiles with length <  20\n",
    "\n",
    " pre-processing step is happening in smi_postprocessing is function.\n",
    " \n",
    " Please note that, biology_flag is served for the situations that the user has an access to biological label.\n",
    "####\n",
    "'''\n",
    "biology_flag = False\n",
    "LogP = [-3,8]\n",
    "MW = [50,600]\n",
    "LB_smi= 20\n",
    "UB_smi = 120\n",
    "txt,data_train_ = smi_postprocessing(data_train,biology_flag,LB_smi,UB_smi,LogP,MW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 49\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Dictionary building part: \n",
    "In this part, we generate dictionary based on the output of the smi_postrocessing function.\n",
    "\n",
    "Then, we shape the data to the usable format for the pytorch using dataset_building function,\n",
    "followed by DataLoader function.\n",
    "\n",
    "'''\n",
    "data_type = 'pure_smiles'\n",
    "char2ind,ind2char,sos_indx,eos_indx,pad_indx = dictionary_build(txt)\n",
    "max_sequence_length = np.max(np.asarray([len(smi) for smi in data_train_['SMILES']]))\n",
    "dataset_train = dataset_building(char2ind,data_train_,max_sequence_length,data_type)\n",
    "dataloader_train = DataLoader(dataset=dataset_train, batch_size= Args.batch_size, shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model loading step: \n",
    "\n",
    "We defined the model with pvae fuunction. reader for more detail of structure of network is encoraged to visit pvae.py.\n",
    "\n",
    "'''\n",
    "vocab_size = len(char2ind) \n",
    "model = SentenceVAE(vocab_size, Args.embedding_size, Args.rnn_type, Args.hidden_size, Args.word_dropout, Args.latent_size,\n",
    "                    sos_indx, eos_indx, pad_indx, max_sequence_length,Args.nr_classes,Args.device_id,\n",
    "                    num_layers=1,bidirectional=False, gpu_exist = Args.gpu_exist)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(Args.device_id)\n",
    "    model = model.cuda(Args.device_id)\n",
    "    \n",
    "class_weight = char_weight(txt,char2ind)\n",
    "class_weight = torch.FloatTensor(class_weight).cuda(device=Args.device_id)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = Args.learning_rate)\n",
    "NLL = torch.nn.CrossEntropyLoss( weight = class_weight, reduction= 'sum', ignore_index=pad_indx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceVAE(\n",
       "  (prediction): Linear(in_features=56, out_features=4, bias=True)\n",
       "  (embedding): Embedding(50, 30)\n",
       "  (word_dropout): Dropout(p=0.1)\n",
       "  (encoder_rnn): GRU(30, 1024, batch_first=True)\n",
       "  (decoder_rnn): GRU(30, 1024, batch_first=True)\n",
       "  (hidden2mean): Linear(in_features=1024, out_features=56, bias=True)\n",
       "  (hidden2logv): Linear(in_features=1024, out_features=56, bias=True)\n",
       "  (latent2hidden): Linear(in_features=56, out_features=1024, bias=True)\n",
       "  (outputs2vocab): Linear(in_features=1024, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e961e20546cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# for anneling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m    \u001b[0;31m# for iteration, batch in enumerate(dataloader_test):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sadegh/sadegh2/anaconda3/envs/pytorch1.1/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "from loss_vae import *\n",
    "tracker = defaultdict(list)\n",
    "for epoch in range(0, Args.epochs):\n",
    "    \n",
    "    optimizer,lr = adjust_learning_rate(optimizer, epoch, Args.learning_rate)\n",
    "    temp = defaultdict(list)\n",
    "    score_acc = []\n",
    "    AUC_acc = []\n",
    "    \n",
    "    for iteration, batch in enumerate(dataloader_train):\n",
    "        \n",
    "        batch = batch2tensor(batch,Args)\n",
    "        batch_size = batch['input'].size(0)\n",
    "        model.train()\n",
    "        ######     Forward pass  ######\n",
    "        logp, mean, logv, z,prediction = model(batch['input'], batch['length'])\n",
    "        prediction = prediction.squeeze()\n",
    "       \n",
    "        NLL_loss, KL_loss, KL_weight = loss_fn(NLL,logp, batch['target'],batch['length'], mean, logv, Args.anneal_function, step, Args.k0,Args.x0)\n",
    "        loss = (NLL_loss + KL_weight * KL_loss )/batch_size\n",
    "   \n",
    "        #### Evaluation #####  \n",
    "        temp['NLL_loss'].append(NLL_loss.item()/batch_size)\n",
    "        temp['KL_loss'].append(KL_weight*KL_loss.item()/batch_size)\n",
    "        temp['ELBO'].append(loss.item())\n",
    "        \n",
    "        #### Backward pass #####\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1 # for anneling\n",
    "   # for iteration, batch in enumerate(dataloader_test):    \n",
    "    #    batch = batch2tensor(batch,Args)\n",
    "     #   batch_size = batch['input'].size(0)\n",
    "      #  model.train()\n",
    "        ######     Forward pass  ######\n",
    "       # logp, mean, logv, z,prediction = model(batch['input'], batch['length'])\n",
    "        #model.eval()\n",
    "\n",
    "    tracker['NLL_loss'].append(np.mean(np.asarray(temp['NLL_loss'])))\n",
    "    tracker['KL_loss'].append(np.mean(np.asarray(temp['KL_loss'])))\n",
    "    tracker['ELBO'].append(np.mean(np.asarray(temp['ELBO'])))\n",
    "    print(\" epoch %04d/%i, 'ELBO' %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f, lr %9.7f\"\n",
    "          %(epoch,Args.epochs, \n",
    "            np.mean(np.asarray(tracker['ELBO'])),\n",
    "            np.mean(np.asarray(tracker['NLL_loss'])),\n",
    "            np.mean(np.asarray(tracker['KL_loss'])),    \n",
    "            KL_weight,\n",
    "            lr))\n",
    "   \n",
    "    \n",
    "    if epoch % Args.save_every == 0:\n",
    "   \n",
    "       \n",
    "        checkpoint_path = os.path.join(Args.save_dir, \"E%i.pytorch\"%(epoch))\n",
    "        torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0000/72, 'ELBO'    8.2686, NLL-Loss    8.0100, KL-Loss    0.2586, KL-Weight  1.000, lr 0.0010000\n"
     ]
    }
   ],
   "source": [
    "tracker['NLL_loss'].append(np.mean(np.asarray(temp['NLL_loss'])))\n",
    "tracker['KL_loss'].append(np.mean(np.asarray(temp['KL_loss'])))\n",
    "tracker['ELBO'].append(np.mean(np.asarray(temp['ELBO'])))\n",
    "print(\" epoch %04d/%i, 'ELBO' %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f, lr %9.7f\"\n",
    "          %(epoch,Args.epochs, \n",
    "            np.mean(np.asarray(tracker['ELBO'])),\n",
    "            np.mean(np.asarray(tracker['NLL_loss'])),\n",
    "            np.mean(np.asarray(tracker['KL_loss'])),    \n",
    "            KL_weight,\n",
    "            lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.1] *",
   "language": "python",
   "name": "conda-env-pytorch1.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
